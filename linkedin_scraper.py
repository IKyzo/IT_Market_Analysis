import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
from urllib.parse import unquote

# Function to scrape data from a specific page
def scrape_page(url):
    response = requests.get(url)

    # Request was successful code: 200
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')

        # Selectors for both featured and regular jobs

        #job_elements_featured = soup.find_all('article', class_='media well listing-item listing-item__jobs listing-item__featured')
        job_elements_regular = soup.find_all('li', class_='ember-view   jobs-search-results__list-item occludable-update p0 relative scaffold-layout__list-item')

        # Combine both types of job elements
        job_elements =  job_elements_regular # + job_elements_featured 

        # Check if any job elements were found
        if job_elements:
            print(f"Found {len(job_elements)} job elements on {url}.")

            # Initialize lists to store data
            job_titles = []
            job_descriptions = []
            company_names = []
            job_locations = []
            #tags = []  # New column for tags

            # Iterating through elements
            for job in job_elements:
                job_title_element = job.find('div', class_="full-width artdeco-entity-lockup__title ember-view")
                #job_description_element = job.find('div', class_='listing-item__desc hidden-sm hidden-xs')
                company_name_element = job.find('div', class_='artdeco-entity-lockup__subtitle ember-view')
                job_location_element = job.find('div', class_='artdeco-entity-lockup__caption ember-view')

                # Determine the tag (featured or available)
                #tag = "featured" if job in job_elements_featured else "available"

                # Extract data and append to lists
                job_titles.append(job_title_element.text if job_title_element else "Title not found")
                #job_descriptions.append(job_description_element.text if job_description_element else "Description not found")
                company_names.append(company_name_element.text if company_name_element else "Company not found")
                job_locations.append(job_location_element.text if job_location_element else "Location not found")
                #tags.append(tag)

            # Create a DataFrame
            data = {'Job Title': job_titles, 'Company': company_names, 'Location': job_locations}
            df = pd.DataFrame(data)

            return df

        else:
            print(f"No job elements found on {url}.")

    else:
        print(f"Failed to retrieve the page {url}. Status Code: {response.status_code}")
        return None

# Main code to iterate through pages
base_url = "https://www.linkedin.com/jobs/search/?currentJobId=3782610403&keywords=jobs%20sousse&origin=BLENDED_SEARCH_RESULT_CARD_NAVIGATION&originToLandingJobPostings=3782610403%2C3775965973%2C3780452120"

# Unquote the URL to replace %20 with an actual space
base_url = unquote(base_url)

# ...

# Set the range of pages you want to scrape
#start_page = 1
#end_page = 10  # Change this to the desired end page

# Initialize an empty DataFrame to store the results
final_df = pd.DataFrame()

for page_number in range(0, 1):
    page_url = f"{base_url}"
    page_df = scrape_page(page_url)

    if page_df is not None:
        final_df = pd.concat([final_df, page_df], ignore_index=True)

# Display the final DataFrame
print(final_df)

# Generate a new filename based on the current date and time
current_datetime = datetime.now().strftime('%Y%m%d_%H%M%S')
new_filename = f'job_data_all_pages_{current_datetime}.xlsx'

# Save the final DataFrame to an Excel file
final_df.to_excel(new_filename, index=False)
